# -*- coding: utf-8 -*-
"""exoplanet-identification-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oVUBx-l2ytyhVxhcGn5NgYnKjwLD7_5x
"""

!pip install catboost

import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

import warnings
warnings.filterwarnings(action='ignore')

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train_use.csv')

data

data.info()

"""# Preprocessing"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd

def preprocess_exoplanet_data(df):
    df = df.copy()

    # Drop unnecessary columns
    columns_to_drop = ['DispositionScore', 'ExoplanetCandidate']
    df = df.drop(columns_to_drop, axis=1)

    # Drop rows with missing values
    df = df.dropna()

    # Split df into X and y
    y = df['ExoplanetConfirmed']
    X = df.drop('ExoplanetConfirmed', axis=1)

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)

    # Scale X
    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)
    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)

    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = preprocess_exoplanet_data(data)

X_train

y_train.value_counts()

"""# Training"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Assuming you have a dataframe named 'df' with your dataset
# Assuming 'ExoplanetConfirmed' is the target column

# Drop unused columns
df = data.drop(['DispositionScore'], axis=1)

# Split df into X and y
y = df['ExoplanetConfirmed']
X = df.drop('ExoplanetConfirmed', axis=1)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)

# Scale X
scaler = StandardScaler()
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)

# Clean column names for XGBoost
X_train_scaled.columns = ["".join (c if c.isalnum() else "_" for c in str(x)) for x in X_train_scaled.columns]
X_test_scaled.columns = ["".join (c if c.isalnum() else "_" for c in str(x)) for x in X_test_scaled.columns]

# Define models
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Neural Network": MLPClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "XGBoost": XGBClassifier(eval_metric='logloss'),
    "LightGBM": LGBMClassifier(),
    "CatBoost": CatBoostClassifier(verbose=0)
}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    print(name + " trained.")

"""# **Results**"""

# Define a function to evaluate and plot precision-recall curves for a given model
def evaluate_model(model, X_test, y_test):
    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate precision and recall for each class
    precision = precision_score(y_test, y_pred, average=None)
    recall = recall_score(y_test, y_pred, average=None)

    # Print precision and recall for each class
    for cls, prec, rec in zip(model.classes_, precision, recall):
        print(f"{name} - Class {cls}: Precision: {prec:.2f}, Recall: {rec:.2f}")

# Evaluate precision-recall for each model
for name, model in models.items():
    evaluate_model(model, X_test_scaled, y_test)

"""# **Results: Plotting**"""

!pip install scikit-learn --upgrade
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt

# Define a function to evaluate and plot ROC and AUC curves for a given model
def evaluate_model(model, X_test, y_test):
    # Binarize the output
    y_test_bin = label_binarize(y_test, classes=model.classes_)

    # Make predictions
    y_pred = model.predict_proba(X_test)

    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(len(model.classes_)):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Plot ROC curve
    plt.figure()
    for i in range(len(model.classes_)):
        plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'{name} - Receiver Operating Characteristic')
    plt.legend(loc="lower right")

    # Show plot
    plt.show()

# Evaluate and plot ROC and AUC curves for each model
for name, model in models.items():
    evaluate_model(model, X_test_scaled, y_test)